{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d479b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f4d58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76ff75",
   "metadata": {},
   "source": [
    "## ðŸ§‘â€ðŸ³ Step 1: Grab the Recipe â€” Load Your Training Configuration\n",
    "\n",
    "Every great dish starts with a recipe â€” and in this notebook, that recipe is your config file.\n",
    "\n",
    "In this step, weâ€™re using Hydra to load a YAML configuration that defines all the key ingredients and settings for your federated fine-tuning experiment. Think of it as pulling out the instruction card before cooking.\n",
    "\n",
    "ðŸ“¦ What this does:\n",
    "- Loads a configuration file (in this case, federated_7b.yml) using Hydra.\n",
    "- Prints out the full config in a readable YAML format, thanks to OmegaConf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "281e1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "def get_config(config_name: str, config_path: str = \"../config/\"):\n",
    "    with initialize(config_path=config_path, version_base=\"1.1\"):\n",
    "        cfg = compose(config_name=config_name)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "def print_config(config: DictConfig):\n",
    "    print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba89bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_config(\"federated_7b.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82427b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  name: medalpaca/medical_meadow_medical_flashcards\n",
      "model:\n",
      "  name: mistralai/Mistral-7B-v0.1\n",
      "  quantization: 4\n",
      "  gradient_checkpointing: true\n",
      "  use_fast_tokenizer: false\n",
      "  lora:\n",
      "    peft_lora_r: 16\n",
      "    peft_lora_alpha: 64\n",
      "    target_modules:\n",
      "    - q_proj\n",
      "    - v_proj\n",
      "train:\n",
      "  num_rounds: ${flower.num_rounds}\n",
      "  save_every_round: 5\n",
      "  learning_rate_max: 5.0e-05\n",
      "  learning_rate_min: 1.0e-06\n",
      "  seq_length: 512\n",
      "  padding_side: left\n",
      "  evaluate_split: true\n",
      "  training_arguments:\n",
      "    output_dir: null\n",
      "    learning_rate: null\n",
      "    per_device_train_batch_size: 16\n",
      "    gradient_accumulation_steps: 1\n",
      "    logging_steps: 10\n",
      "    num_train_epochs: 3\n",
      "    max_steps: 10\n",
      "    report_to: null\n",
      "    save_steps: 1000\n",
      "    save_total_limit: 10\n",
      "    gradient_checkpointing: ${model.gradient_checkpointing}\n",
      "    lr_scheduler_type: constant\n",
      "client_resources:\n",
      "  num_cpus: 8\n",
      "  num_gpus: 1.0\n",
      "dp:\n",
      "  noise_mult: 0.02\n",
      "  clip_norm: 0.5\n",
      "flower:\n",
      "  num_clients: 20\n",
      "  num_rounds: 200\n",
      "  fraction_fit: 0.8\n",
      "  client_resources:\n",
      "    num_cpus: 8\n",
      "    num_gpus: 1.0\n",
      "  dp:\n",
      "    noise_mult: 0.02\n",
      "    clip_norm: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d074b",
   "metadata": {},
   "source": [
    "## ðŸ§‘â€ðŸ³ Step 2: Inspect Your Ingredients â€” Visualize the Dataset Partitions\n",
    "\n",
    "Now that we have our recipe config, letâ€™s take a look at the ingredients weâ€™ll be cooking with.\n",
    "\n",
    "In this step, we load a federated dataset using flwr_datasets and plot how the training data is split across different partitions (or â€œclientsâ€). Think of this as checking how much of each ingredient each chef (or server) gets!\n",
    "\n",
    "ðŸ“ What this does:\n",
    "- Loads one partition to trigger dataset setup.\n",
    "- Determines how many partitions (or clients) weâ€™re working with.\n",
    "- Plots a bar chart to show how many samples are in each clientâ€™s slice of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdbb33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr_datasets import FederatedDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_partitions(fed_dataset: FederatedDataset):\n",
    "    _ = fed_dataset.load_partition(0)\n",
    "    num_partitions = fed_dataset.partitioners['train'].num_partitions\n",
    "    \n",
    "    plt.bar(range(num_partitions), [len(fed_dataset.load_partition(i)) for i in range(num_partitions)])\n",
    "    plt.xticks(range(num_partitions))\n",
    "    plt.xlabel(\"Partition ID\")\n",
    "    plt.ylabel(\"Number of examples\")\n",
    "    plt.title(f\"IID partitioning into {num_partitions} partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e044507",
   "metadata": {},
   "source": [
    "## Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa99ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from peft.utils import prepare_model_for_kbit_training\n",
    "from trl import DataCollatorForCompletionOnlyLM, SFTTrainer\n",
    "\n",
    "def get_model(model_cfg: DictConfig):\n",
    "    \"\"\"Load model with appropiate quantization config and\n",
    "    other optimizations.\"\"\"\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    quantization_config = None\n",
    "    model_name = model_cfg.name\n",
    "    if use_cuda:\n",
    "        if model_cfg.quantization == 4:\n",
    "            quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "        elif model_cfg.quantization == 8:\n",
    "            quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Use 4-bit or 8-bit quantization. You passed: {model_cfg.quantization}/\"\n",
    "            )\n",
    "\n",
    "        model_name = model_cfg.name\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "\n",
    "    if use_cuda:\n",
    "        model = prepare_model_for_kbit_training(\n",
    "            model, use_gradient_checkpointing=model_cfg.gradient_checkpointing\n",
    "        )\n",
    "\n",
    "    target_modules = model_cfg.lora.target_modules\n",
    "    if target_modules:\n",
    "        target_modules = list(target_modules)\n",
    "    peft_config = LoraConfig(\n",
    "        r=model_cfg.lora.peft_lora_r,\n",
    "        lora_alpha=model_cfg.lora.peft_lora_alpha,\n",
    "        lora_dropout=0.075,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "    if not (use_cuda):\n",
    "        peft_model.enable_input_require_grads()\n",
    "\n",
    "    if model_cfg.gradient_checkpointing:\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    return peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41691bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_communication_costs(config, comm_bw_mbps: float = 20):\n",
    "    model = get_model(config.model)\n",
    "\n",
    "    trainable, all_parameters = model.get_nb_trainable_parameters()\n",
    "\n",
    "    total_size = 4*all_parameters/(1024**2)\n",
    "    trainable_size = 4*trainable/(1024**2)\n",
    "\n",
    "    upload_time_total = total_size/(comm_bw_mbps/8)\n",
    "    upload_time_finetune = trainable_size/(comm_bw_mbps/8)\n",
    "    \n",
    "    print(f\"Full model:\\n\\t{all_parameters/1e6:.3f} M parameters\\n\\t{total_size:.2f} MB --> upload in {upload_time_total:.2f}s @ {comm_bw_mbps}Mbps\")\n",
    "    print(f\"Finetuned model:\\n\\t{trainable/1e6:.3f} M parameters\\n\\t{trainable_size:.2f} MB --> upload in {upload_time_finetune:.2f}s @ {comm_bw_mbps}Mbps\")\n",
    "    # print(f\"In a {comm_bw_mbps} Mbps channel --> {}\")\n",
    "\n",
    "    num_rounds = config.flower.num_rounds\n",
    "    num_clients_per_round = int(config.flower.num_clients * config.flower.fraction_fit)\n",
    "    print(f\"Federated Learning setting: \"\n",
    "          f\"\\n\\tNumber of rounds: {num_rounds}\"\n",
    "          f\"\\n\\tNumber of clients per round: {num_clients_per_round}\")\n",
    "    \n",
    "    print(f\"-----------------------------------------------\")\n",
    "    print(f\"Total Communication costs (Full model): {2*num_rounds*num_clients_per_round*total_size/1024:.1f} GB\")\n",
    "    print(f\"Total Communication costs (Finetuning): {2*num_rounds*num_clients_per_round*trainable_size} MB\")\n",
    "    print(f\"Communication savings: {all_parameters/trainable:.1f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
